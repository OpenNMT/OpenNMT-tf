# The directory where models and summaries will be saved. It is created if it does not exist.
model_dir: wmt_ende_transformer_4gpu_lr2_ws8000_dur2_0.998

data:
  train_features_file: data/train.en
  train_labels_file: data/train.de
  eval_features_file: data/valid.en
  eval_labels_file: data/valid.de
  source_words_vocabulary: data/wmtende.vocab
  target_words_vocabulary: data/wmtende.vocab

params:
  beam_width: 5
  maximum_iterations: 250
  average_loss_in_time: true
  label_smoothing: 0.1
  length_penalty: 0.6

  optimizer: LazyAdamOptimizer
  optimizer_params:
    beta1: 0.9
    beta2: 0.998
  learning_rate: 2.0  # The learning rate scale constant.
  decay_type: noam_decay
  decay_rate: 512  # Model dimension.
  decay_step_duration: 2  # 1 decay step is 2 training steps.
  decay_steps: 8000 # Warmup steps (= 16000 training steps).
  start_decay_steps: 0

train:
  batch_size: 4096
  batch_type: tokens
  bucket_width: 1
  save_checkpoints_steps: 1000
  keep_checkpoint_max: 10
  save_summary_steps: 50
  train_steps: 500000
  maximum_features_length: 100
  maximum_labels_length: 100

  # Uniformly shuffle accross the training data.
  # Set a fixed value if you are running low on system memory.
  sample_buffer_size: -1

eval:
  batch_size: 32
  eval_delay: 3600  # Every 1 hour
  external_evaluators: BLEU

infer:
  batch_size: 32
