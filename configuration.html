<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Parameters &mdash; OpenNMT-tf 2.30.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Data" href="data.html" />
    <link rel="prev" title="Model" href="model.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> OpenNMT-tf
            <img src="_static/logo-alpha.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2.30
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Configuration</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="model.html">Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#automatic-configuration">Automatic configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multiple-configuration-files">Multiple configuration files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="vocabulary.html">Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignments.html">Alignments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving.html">Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="v2_transition.html">2.0 Transition Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="package/overview.html">Python</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-tf</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Parameters</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="parameters">
<h1>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline"></a></h1>
<p>Run parameters are described in separate YAML files. They define data files, optimization settings, dynamic model parameters, and options related to training and inference. It uses the following layout:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">data</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Data configuration (training and evaluation files, vocabularies, alignments, etc.)</span><span class="w"></span>
<span class="nt">params</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Training and inference hyperparameters (learning rate, optimizer, beam size, etc.)</span><span class="w"></span>
<span class="nt">train</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Training specific configuration (checkpoint frequency, number of training step, etc.)</span><span class="w"></span>
<span class="nt">eval</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Evaluation specific configuration (evaluation frequency, scorers, etc.)</span><span class="w"></span>
<span class="nt">infer</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Inference specific configuration (output scores, alignments, etc.)</span><span class="w"></span>
<span class="nt">score</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Scoring specific configuration</span><span class="w"></span>
</pre></div>
</div>
<section id="automatic-configuration">
<h2>Automatic configuration<a class="headerlink" href="#automatic-configuration" title="Permalink to this headline"></a></h2>
<p>Predefined models declare default parameters that should give solid performance out of the box. To enable automatic configuration, use the <code class="docutils literal notranslate"><span class="pre">--auto_config</span></code> flag:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-main --model_type Transformer --config my_data.yml --auto_config train
</pre></div>
</div>
<p>The user provided <code class="docutils literal notranslate"><span class="pre">my_data.yml</span></code> file will minimally require the data configuration (see <a class="reference internal" href="quickstart.html"><span class="doc std std-doc">Quickstart</span></a> for example). You might want to also configure checkpoint related settings, the logging frequency, and the number of training steps.</p>
<p>At the start of the training, the configuration values actually used will be logged. If you want to change some of them, simply add the parameter in your configuration file to override the default value.</p>
<p><strong>Note:</strong> default training values usually assume GPUs with at least 8GB of memory and a large system memory:</p>
<ul class="simple">
<li><p>If you encounter GPU out of memory issues, try overriding <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> to a lower value.</p></li>
<li><p>If you encounter CPU out of memory issues, try overriding <code class="docutils literal notranslate"><span class="pre">sample_buffer_size</span></code> to a fixed value.</p></li>
</ul>
</section>
<section id="multiple-configuration-files">
<h2>Multiple configuration files<a class="headerlink" href="#multiple-configuration-files" title="Permalink to this headline"></a></h2>
<p>The command line accepts multiple configuration files so that some parts can be made reusable, e.g:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-main --config config/opennmt-defaults.yml config/optim/adam_with_decay.yml <span class="se">\</span>
    config/data/toy-ende.yml <span class="o">[</span>...<span class="o">]</span>
</pre></div>
</div>
<p>If a configuration key is duplicated, the value defined in the rightmost configuration file has priority.</p>
<p>If you are unsure about the configuration that is actually used or simply prefer working with a single file, consider using the <code class="docutils literal notranslate"><span class="pre">merge_config</span></code> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-merge-config config/opennmt-defaults.yml config/optim/adam_with_decay.yml <span class="se">\</span>
    config/data/toy-ende.yml &gt; config/my_config.yml
</pre></div>
</div>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h2>
<p>Below is an exhaustive and documented configuration. <strong>You should NOT copy and use this configuration, instead you should only define the parameters that you need.</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># (optional) The directory where models and summaries will be saved.</span><span class="w"></span>
<span class="c1"># Can also be set with the command line option --model_dir.</span><span class="w"></span>
<span class="c1"># The directory is created if it does not exist.</span><span class="w"></span>
<span class="nt">model_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende</span><span class="w"></span>

<span class="c1"># (optional) Enable automatic parameters based on the selected model.</span><span class="w"></span>
<span class="c1"># Can also be set with the command line option --auto_config.</span><span class="w"></span>
<span class="nt">auto_config</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>

<span class="nt">data</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (required for train run type).</span><span class="w"></span>
<span class="w">  </span><span class="nt">train_features_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/src-train.txt</span><span class="w"></span>
<span class="w">  </span><span class="nt">train_labels_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/tgt-train.txt</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) A list with the weights of each training files, if multiple training</span><span class="w"></span>
<span class="w">  </span><span class="c1"># files were configured (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">train_files_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Pharaoh alignments of the training files.</span><span class="w"></span>
<span class="w">  </span><span class="nt">train_alignments</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/alignments-train.txt</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) File containing the weight of each example (one weight per line).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># The loss value of each example is multiplied by its corresponding weight.</span><span class="w"></span>
<span class="w">  </span><span class="nt">example_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/weights-train.txt</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (required for train_end_eval and eval run types).</span><span class="w"></span>
<span class="w">  </span><span class="nt">eval_features_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/src-val.txt</span><span class="w"></span>
<span class="w">  </span><span class="nt">eval_labels_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/tgt-val.txt</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Models may require additional resource files (e.g. vocabularies).</span><span class="w"></span>
<span class="w">  </span><span class="nt">source_vocabulary</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/src-vocab.txt</span><span class="w"></span>
<span class="w">  </span><span class="nt">target_vocabulary</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/toy-ende/tgt-vocab.txt</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) During export save the vocabularies as model assets, otherwise embed</span><span class="w"></span>
<span class="w">  </span><span class="c1"># them in the graph itself (default: true).</span><span class="w"></span>
<span class="w">  </span><span class="nt">export_vocabulary_assets</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Tokenization configuration (or path to a configuration file).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># See also: https://github.com/OpenNMT/Tokenizer/blob/master/docs/options.md</span><span class="w"></span>
<span class="w">  </span><span class="nt">source_tokenization</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OpenNMTTokenizer</span><span class="w"></span>
<span class="w">    </span><span class="nt">params</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aggressive</span><span class="w"></span>
<span class="w">      </span><span class="nt">joiner_annotate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">      </span><span class="nt">segment_numbers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">      </span><span class="nt">segment_alphabet_change</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">  </span><span class="nt">target_tokenization</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">config/tokenization/aggressive.yml</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Pretrained embedding configuration.</span><span class="w"></span>
<span class="w">  </span><span class="nt">source_embedding</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/glove/glove-100000.txt</span><span class="w"></span>
<span class="w">    </span><span class="nt">with_header</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">    </span><span class="nt">case_insensitive</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">    </span><span class="nt">trainable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) For language models, configure sequence control tokens (usually</span><span class="w"></span>
<span class="w">  </span><span class="c1"># represented as &lt;s&gt; and &lt;/s&gt;). For example, enabling &quot;start&quot; and disabling &quot;end&quot;</span><span class="w"></span>
<span class="w">  </span><span class="c1"># allows nonconditional and unbounded generation (default: start=false, end=true).</span><span class="w"></span>
<span class="w">  </span><span class="c1">#</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Advanced users could also configure this parameter for seq2seq models with e.g.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># source_sequence_controls and target_sequence_controls.</span><span class="w"></span>
<span class="w">  </span><span class="nt">sequence_controls</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">start</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">    </span><span class="nt">end</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) For sequence tagging tasks, the tagging scheme that is used (e.g. BIOES).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># For supported schemes, additional evaluation metrics could be computed such as</span><span class="w"></span>
<span class="w">  </span><span class="c1"># precision, recall, etc. (accepted values: bioes; default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">tagging_scheme</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bioes</span><span class="w"></span>

<span class="c1"># Model and optimization parameters.</span><span class="w"></span>
<span class="nt">params</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># The optimizer class name in tf.keras.optimizers or tfa.optimizers.</span><span class="w"></span>
<span class="w">  </span><span class="nt">optimizer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Adam</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Additional optimizer parameters as defined in their documentation.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># If weight_decay is set, the optimizer will be extended with decoupled weight decay.</span><span class="w"></span>
<span class="w">  </span><span class="nt">optimizer_params</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">beta_1</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.8</span><span class="w"></span>
<span class="w">    </span><span class="nt">beta_2</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.998</span><span class="w"></span>
<span class="w">  </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) If set, overrides all dropout values configured in the model definition.</span><span class="w"></span>
<span class="w">  </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.3</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) List of layer to not optimize.</span><span class="w"></span>
<span class="w">  </span><span class="nt">freeze_layers</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;encoder/layers/0&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;decoder/output_layer&quot;</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Weights regularization penalty (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">regularization</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">l2</span><span class="w">  </span><span class="c1"># can be &quot;l1&quot;, &quot;l2&quot;, &quot;l1_l2&quot; (case-insensitive).</span><span class="w"></span>
<span class="w">    </span><span class="nt">scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-4</span><span class="w">  </span><span class="c1"># if using &quot;l1_l2&quot; regularization, this should be a YAML list.</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Average loss in the time dimension in addition to the batch dimension</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (default: true when using &quot;tokens&quot; batch type, false otherwise).</span><span class="w"></span>
<span class="w">  </span><span class="nt">average_loss_in_time</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) High training loss values considered as outliers will be masked (default: false).</span><span class="w"></span>
<span class="w">  </span><span class="nt">mask_loss_outliers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The type of learning rate decay (default: null). See:</span><span class="w"></span>
<span class="w">  </span><span class="c1">#  * https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules</span><span class="w"></span>
<span class="w">  </span><span class="c1">#  * https://opennmt.net/OpenNMT-tf/package/opennmt.schedules.html</span><span class="w"></span>
<span class="w">  </span><span class="c1"># This value may change the semantics of other decay options. See the documentation</span><span class="w"></span>
<span class="w">  </span><span class="c1"># or the code.</span><span class="w"></span>
<span class="w">  </span><span class="nt">decay_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoamDecay</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional unless decay_type is set) Decay parameters.</span><span class="w"></span>
<span class="w">  </span><span class="nt">decay_params</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">model_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"></span>
<span class="w">    </span><span class="nt">warmup_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4000</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The number of training steps that make 1 decay step (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">decay_step_duration</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) After how many steps to start the decay (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">start_decay_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50000</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The learning rate minimum value (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">minimum_learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0001</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Type of scheduled sampling (can be &quot;constant&quot;, &quot;linear&quot;, &quot;exponential&quot;,</span><span class="w"></span>
<span class="w">  </span><span class="c1"># or &quot;inverse_sigmoid&quot;, default: &quot;constant&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">scheduled_sampling_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">constant</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Probability to read directly from the inputs instead of sampling categorically</span><span class="w"></span>
<span class="w">  </span><span class="c1"># from the output ids (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">scheduled_sampling_read_probability</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional unless scheduled_sampling_type is set) The constant k of the schedule.</span><span class="w"></span>
<span class="w">  </span><span class="nt">scheduled_sampling_k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The label smoothing value.</span><span class="w"></span>
<span class="w">  </span><span class="nt">label_smoothing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Width of the beam search (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">beam_width</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Number of hypotheses to return (default: 1). Set 0 to return all</span><span class="w"></span>
<span class="w">  </span><span class="c1"># available hypotheses. This value is also set by infer/n_best.</span><span class="w"></span>
<span class="w">  </span><span class="nt">num_hypotheses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Length penaly weight to use during beam search (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">length_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Coverage penaly weight to use during beam search (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">coverage_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Sample predictions from the top K most likely tokens (requires</span><span class="w"></span>
<span class="w">  </span><span class="c1"># beam_width to 1). If 0, sample from the full output distribution (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">sampling_topk</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) High temperatures generate more random samples (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">sampling_temperature</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Sequence of noise to apply to the decoding output. Each element</span><span class="w"></span>
<span class="w">  </span><span class="c1"># should be a noise type (can be: &quot;dropout&quot;, &quot;replacement&quot;, &quot;permutation&quot;) and</span><span class="w"></span>
<span class="w">  </span><span class="c1"># the module arguments</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (see https://opennmt.net/OpenNMT-tf/package/opennmt.data.noise.html)</span><span class="w"></span>
<span class="w">  </span><span class="nt">decoding_noise</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.1</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">｟unk｠</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">permutation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Define the subword marker. This is useful to apply noise at the</span><span class="w"></span>
<span class="w">  </span><span class="c1"># word level instead of the subword level (default: ￭).</span><span class="w"></span>
<span class="w">  </span><span class="nt">decoding_subword_token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">￭</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Whether decoding_subword_token is used as a spacer (as in SentencePiece)</span><span class="w"></span>
<span class="w">  </span><span class="c1"># or a joiner (as in BPE).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># If unspecified, will infer  directly from decoding_subword_token.</span><span class="w"></span>
<span class="w">  </span><span class="nt">decoding_subword_token_is_spacer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Minimum length of decoded sequences, end token excluded (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">minimum_decoding_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Maximum length of decoded sequences, end token excluded (default: 250).</span><span class="w"></span>
<span class="w">  </span><span class="nt">maximum_decoding_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">250</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Replace unknown target tokens by the original source token with the</span><span class="w"></span>
<span class="w">  </span><span class="c1"># highest attention (default: false).</span><span class="w"></span>
<span class="w">  </span><span class="nt">replace_unknown_target</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The type of guided alignment cost to compute (can be: &quot;null&quot;, &quot;ce&quot;, &quot;mse&quot;,</span><span class="w"></span>
<span class="w">  </span><span class="c1"># default: &quot;null&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">guided_alignment_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The weight of the guided alignment cost (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">guided_alignment_weight</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Enable contrastive learning mode, see</span><span class="w"></span>
<span class="w">  </span><span class="c1"># https://www.aclweb.org/anthology/P19-1623 (default: false).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># See also &quot;decoding_subword_token&quot; that is used by this mode.</span><span class="w"></span>
<span class="w">  </span><span class="nt">contrastive_learning</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The value of the parameter eta in the max-margin loss (default: 0.1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">max_margin_eta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Size of output on an exported TensorFlow Lite model</span><span class="w"></span>
<span class="w">  </span><span class="nt">tflite_output_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">250</span><span class="w"></span>


<span class="c1"># Training options.</span><span class="w"></span>
<span class="nt">train</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Training batch size. If set to 0, the training will search the largest</span><span class="w"></span>
<span class="w">  </span><span class="c1"># possible batch size.</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Batch size is the number of &quot;examples&quot; or &quot;tokens&quot; (default: &quot;examples&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Tune gradient accumulation to train with at least this effective batch size</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">effective_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">25000</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Save a checkpoint every this many steps (default: 5000).</span><span class="w"></span>
<span class="w">  </span><span class="nt">save_checkpoints_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) How many checkpoints to keep on disk.</span><span class="w"></span>
<span class="w">  </span><span class="nt">keep_checkpoint_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Dump summaries and logs every this many steps (default: 100).</span><span class="w"></span>
<span class="w">  </span><span class="nt">save_summary_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Maximum training step. If not set, train forever.</span><span class="w"></span>
<span class="w">  </span><span class="nt">max_step</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000000</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) If true, makes a single pass over the training data (default: false).</span><span class="w"></span>
<span class="w">  </span><span class="nt">single_pass</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The maximum length of feature sequences during training (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">maximum_features_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">70</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The maximum length of label sequences during training (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">maximum_labels_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">70</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The width of the length buckets to select batch candidates from.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># A smaller value means less padding and increased efficiency. (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="nt">length_bucket_width</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The number of elements from which to sample during shuffling (default: 500000).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Set 0 or null to disable shuffling, -1 to match the number of training examples.</span><span class="w"></span>
<span class="w">  </span><span class="nt">sample_buffer_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500000</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Moving average decay. Reasonable values are close to 1, e.g. 0.9999, see</span><span class="w"></span>
<span class="w">  </span><span class="c1"># https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (default: null)</span><span class="w"></span>
<span class="w">  </span><span class="nt">moving_average_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9999</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Number of checkpoints to average at the end of the training to the directory</span><span class="w"></span>
<span class="w">  </span><span class="c1"># model_dir/avg (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">average_last_checkpoints</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"></span>


<span class="c1"># (optional) Evaluation options.</span><span class="w"></span>
<span class="nt">eval</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The batch size to use (default: 32).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Batch size is the number of &quot;examples&quot; or &quot;tokens&quot; (default: &quot;examples&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Evaluate every this many steps (default: 5000).</span><span class="w"></span>
<span class="w">  </span><span class="nt">steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5000</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Save evaluation predictions in model_dir/eval/.</span><span class="w"></span>
<span class="w">  </span><span class="nt">save_eval_predictions</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Scorer or list of scorers that are called on the saved evaluation</span><span class="w"></span>
<span class="w">  </span><span class="c1"># predictions.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Available scorers: bleu, rouge, wer, ter, prf, chrf, chrf++</span><span class="w"></span>
<span class="w">  </span><span class="nt">scorers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bleu</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The width of the length buckets to select batch candidates from.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># If set, the eval data will be sorted by length to increase the translation</span><span class="w"></span>
<span class="w">  </span><span class="c1"># efficiency. The predictions will still be outputted in order as they are</span><span class="w"></span>
<span class="w">  </span><span class="c1"># available (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">length_bucket_width</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Export a model when a metric has the best value so far (default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">export_on_best</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bleu</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Format of the exported model (can be: &quot;saved_model, &quot;checkpoint&quot;,</span><span class="w"></span>
<span class="w">  </span><span class="c1"># &quot;ctranslate2&quot;, &quot;ctranslate2_int8&quot;, &quot;ctranslate2_int16&quot;, &quot;ctranslate2_float16&quot;,</span><span class="w"></span>
<span class="w">  </span><span class="c1"># default: &quot;saved_model&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">export_format</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">saved_model</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Maximum number of exports to keep on disk (default: 5).</span><span class="w"></span>
<span class="w">  </span><span class="nt">max_exports_to_keep</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Early stopping condition.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Should be read as: stop the training if &quot;metric&quot; did not improve more</span><span class="w"></span>
<span class="w">  </span><span class="c1"># than &quot;min_improvement&quot; in the last &quot;steps&quot; evaluations.</span><span class="w"></span>
<span class="w">  </span><span class="nt">early_stopping</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="c1"># (optional) The target metric name (default: &quot;loss&quot;).</span><span class="w"></span>
<span class="w">    </span><span class="nt">metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bleu</span><span class="w"></span>
<span class="w">    </span><span class="c1"># (optional) The metric should improve at least by this much to be considered</span><span class="w"></span>
<span class="w">    </span><span class="c1"># as an improvement (default: 0)</span><span class="w"></span>
<span class="w">    </span><span class="nt">min_improvement</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.01</span><span class="w"></span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"></span>

<span class="c1"># (optional) Inference options.</span><span class="w"></span>
<span class="nt">infer</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The batch size to use (default: 16).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Batch size is the number of &quot;examples&quot; or &quot;tokens&quot; (default: &quot;examples&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) For compatible models, the number of hypotheses to output (default: 1).</span><span class="w"></span>
<span class="w">  </span><span class="c1"># This sets the parameter params/num_hypotheses.</span><span class="w"></span>
<span class="w">  </span><span class="nt">n_best</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) For compatible models, also output the score (default: false).</span><span class="w"></span>
<span class="w">  </span><span class="nt">with_scores</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) For compatible models, also output the alignments</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (can be: null, hard, soft, default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">with_alignments</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The width of the length buckets to select batch candidates from.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># If set, the test data will be sorted by length to increase the translation</span><span class="w"></span>
<span class="w">  </span><span class="c1"># efficiency. The predictions will still be outputted in order as they are</span><span class="w"></span>
<span class="w">  </span><span class="c1"># available (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">length_bucket_width</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>


<span class="c1"># (optional) Scoring options.</span><span class="w"></span>
<span class="nt">score</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) The batch size to use (default: 64).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Batch size is the number of &quot;examples&quot; or &quot;tokens&quot; (default: &quot;examples&quot;).</span><span class="w"></span>
<span class="w">  </span><span class="nt">batch_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) The width of the length buckets to select batch candidates from.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># If set, the input file will be sorted by length to increase efficiency.</span><span class="w"></span>
<span class="w">  </span><span class="c1"># The result will still be outputted in order as they are available (default: 0).</span><span class="w"></span>
<span class="w">  </span><span class="nt">length_bucket_width</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>

<span class="w">  </span><span class="c1"># (optional) Also report token-level cross entropy.</span><span class="w"></span>
<span class="w">  </span><span class="nt">with_token_level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">  </span><span class="c1"># (optional) Also output the alignments (can be: null, hard, soft, default: null).</span><span class="w"></span>
<span class="w">  </span><span class="nt">with_alignments</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="model.html" class="btn btn-neutral float-left" title="Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data.html" class="btn btn-neutral float-right" title="Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>