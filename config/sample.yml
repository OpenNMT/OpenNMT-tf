# WARNING:
# Do not use this file as is! It only lists and documents available options
# without caring about their consistency. Prefer using "opennmt-defaults.yml" as a
# base configuration and specialize it with additional configuration files.

# The directory where models and summaries will be saved. It is created if it does not exist.
model_dir: toy-ende

data:
  train_features_file: data/toy-ende/src-train.txt
  train_labels_file: data/toy-ende/tgt-train.txt
  eval_features_file: data/toy-ende/src-val.txt
  eval_labels_file: data/toy-ende/tgt-val.txt

  # (optional) Models may require additional resource files (e.g. vocabularies).
  source_words_vocabulary: data/toy-ende/src-vocab.txt
  target_words_vocabulary: data/toy-ende/tgt-vocab.txt

# Model and optimization parameters.
params:
  # The optimizer class name in tf.train or tf.contrib.opt.
  optimizer: AdamOptimizer
  learning_rate: 1.0

  # (optional) Global parameter initialization [-param_init, param_init].
  param_init: 0.1
  # (optional) Maximum gradients norm (default: None).
  clip_gradients: 5.0
  # (optional) Average loss in the time dimension in addition to the batch dimension (default: False).
  average_loss_in_time: false
  # (optional) The type of learning rate decay (default: None). See:
  #  * https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate
  #  * opennmt/utils/decay.py
  # This value may change the semantics of other decay options. See the documentation or the code.
  decay_type: exponential_decay
  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.7
  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 10000
  # (optional) If true, the learning rate is decayed in a staircase fashion (default: True).
  staircase: true
  # (optional) After how many steps to start the decay (default: 0).
  start_decay_steps: 50000
  # (optional) Stop decay when this learning rate value is reached (default: 0).
  minimum_learning_rate: 0.0001
  # (optional) Type of scheduled sampling (can be "constant", "linear", "exponential",
  # or "inverse_sigmoid", default: "constant").
  scheduled_sampling_type: constant
  # (optional) Probability to read directly from the inputs instead of sampling categorically
  # from the output ids (default: 1).
  scheduled_sampling_read_probability: 1
  # (optional unless scheduled_sampling_type is set) The constant k of the schedule.
  scheduled_sampling_k: 0
  # (optional) The label smoothing value.
  label_smoothing: 0.1
  # (optional) Width of the beam search (default: 1).
  beam_width: 5
  # (optional) Length penaly weight to apply on hypotheses (default: 0).
  length_penalty: 0.2
  # (optional) Maximum decoding iterations before stopping (default: 250).
  maximum_iterations: 200

# Training options.
train:
  batch_size: 64

  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples
  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 5000
  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3
  # (optional) Save summaries every this many steps.
  save_summary_steps: 100
  # (optional) Train for this many steps. If not set, train forever.
  train_steps: 1000000
  # (optional) The maximum length of feature sequences during training (default: None).
  maximum_features_length: 70
  # (optional) The maximum length of label sequences during training (default: None).
  maximum_labels_length: 70
  # (optional) The width of the length buckets to select batch candidates from (default: 5).
  bucket_width: 5
  # (optional) The number of threads to use for processing data in parallel (default: number of logical cores).
  num_parallel_process_calls: 4
  # (optional) The data prefetch buffer size (default: batch_size * 1000).
  prefetch_buffer_size: 10000
  # (optional) The number of elements from which to sample during shuffling (default: 1000000).
  # Consider setting this to the number of training examples.
  sample_buffer_size: 1000000

# (optional) Evaluation options.
eval:
  # (optional) The batch size to use (default: train value or 30 if not applicable).
  batch_size: 30
  # (optional) The number of threads to use for processing data in parallel (default: train value).
  num_parallel_process_calls: 4
  # (optional) The prefetch buffer size when processing data in parallel (default: batch_size * 10).
  prefetch_buffer_size: 100
  # (optional) Evaluate every this many seconds (default: 18000).
  eval_delay: 7200
  # (optional) Save evaluation predictions in model_dir/eval/.
  save_eval_predictions: false
  # (optional) Evalutator or list of evaluators that are called on the saved evaluation predictions.
  # Available evaluators: BLEU, BLEU-detok
  external_evaluators: BLEU

# (optional) Inference options.
infer:
  # (optional) The batch size to use (default: 1).
  batch_size: 10
  # (optional) The number of threads to use for processing data in parallel (default: number of logical cores).
  num_parallel_process_calls: 4
  # (optional) The prefetch buffer size when processing data in parallel (default: batch_size * 10).
  prefetch_buffer_size: 100
  # (optional) For compatible models, the number of hypotheses to output (default: 1).
  n_best: 1
