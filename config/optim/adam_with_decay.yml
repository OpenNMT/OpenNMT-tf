params:
  optimizer: AdamOptimizer
  learning_rate: 0.0002
  clip_gradients: 5.0
  decay_type: exponential_decay
  decay_rate: 0.7
  decay_steps: 100000
  start_decay_steps: 500000
